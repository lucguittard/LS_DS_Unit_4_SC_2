{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** the units in a neural network that recieve inputs from other nodes or an external source to compute an output\n",
    "- **Input Layer:** This is the data collected for each of the independent variables (x1, x2, x... , or X)\n",
    "- **Hidden Layer:** These are layers containing different itterations of an activation function; inputs are fed to this layer and assigned different weights accross a span of nodes/neurons/activation functions to determine the makeup of the output layer.\n",
    "- **Output Layer:** This is the layer that transmits information from the neural network to the user\n",
    "- **Activation:** This is the source of non-linearity in the outputs of a neuron/node and determines what gets passed to latter layers of a neural-network\n",
    "- **Backpropagation:** refers to a method of training a neural network; it involves the use of an algorithm to update the weights being used which get updated in reverse order after a training epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5000\n",
       "0    5000\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.ate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = X.shape[1]\n",
    "\n",
    "weights = np.zeros(1 + n_features)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.01\n",
    "niter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(niter):\n",
    "    err = 0\n",
    "    #print('Iteration:', i, '\\n')\n",
    "    for xi, target in zip(X, y):\n",
    "        #print('Inputs')\n",
    "        #print('Row {} feature values:'.format(i), xi, '\\nRow {} target class:'.format(i), target, '\\n')\n",
    "        #print('Initial Operations')\n",
    "        #print('Dot product of feature values and weights:', np.dot(xi, weights[1:]))\n",
    "        net_input = np.dot(xi, weights[1:]) + weights[0]\n",
    "        #print('Sum of dot product and our bias (aka net input):', net_input, '\\n')\n",
    "        prediction = np.where(net_input >= 0.0, 1, -1)\n",
    "        #print('Prediction:', prediction, '\\n')\n",
    "        #print('Update Weights and Bias')\n",
    "        delta_w = rate * (target - prediction)\n",
    "        #print('Current weights:', weights[1:])\n",
    "        #print('Weight change:', delta_w)\n",
    "        weights[1:] += delta_w * xi\n",
    "        #print('Updated weights:', weights[1:])\n",
    "        #print('Current bias term:', weights[0])\n",
    "        weights[0] += delta_w\n",
    "        #print('Updated bias term:', weights[0])\n",
    "        err += int(delta_w != 0.0)\n",
    "        #print('--------------------\\n')\n",
    "    #print('************************************************\\n\\n')\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6268 0.3732\n"
     ]
    }
   ],
   "source": [
    "best = min(errors)\n",
    "prop_miss = 1 - (best)/len(X)\n",
    "print(best, prop_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(errors) + 1), errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: less than 50% (37.52%)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Explain why you could not achieve a higher accuracy with the simple perceptron architecture, because it's possible to achieve ~95% accuracy on this dataset.\n",
    "\n",
    "The above basic perceptron couldn't be improved beyond the low accuracy identified. As is seen in the plot just above, its predictive performance bottoms out at around 37.52 percent (vs. a 50% baseline). This might be attributed to a failing to build on the measures of error to improve the model. One method to do this is through backpropogation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float64')\n",
    "y = y.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = np.reshape(y, (10000,1))\n",
    "y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.input = 2\n",
    "        self.hiddenNodes = 6 \n",
    "        self.outputNodes = 1\n",
    "        \n",
    "        # Initial Weights\n",
    "        self.weights1 = np.random.randn(self.input,self.hiddenNodes)\n",
    "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    \n",
    "    def feed_forward(self,X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs & hidden\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weighted sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final Activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "        \n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "        \n",
    "        \n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "output [0.55716785]\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "print(X[0])\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44283215])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y3[0] - output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.55716785]\n",
      " [0.26079709]\n",
      " [0.55716785]\n",
      " ...\n",
      " [0.55716785]\n",
      " [0.55716785]\n",
      " [0.26079709]]\n",
      "[[0.44283215]\n",
      " [0.73920291]\n",
      " [0.44283215]\n",
      " ...\n",
      " [0.44283215]\n",
      " [0.44283215]\n",
      " [0.73920291]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y3 - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activated_hidden\n",
      " [[9.48171804e-01 2.69593186e-01 3.77105889e-01 4.96529264e-03\n",
      "  9.99854925e-01 6.30095853e-01]\n",
      " [1.16362016e-01 9.99125359e-01 9.99753620e-01 1.23833727e-04\n",
      "  1.00000000e+00 1.67495417e-01]\n",
      " [9.48171804e-01 2.69593186e-01 3.77105889e-01 4.96529264e-03\n",
      "  9.99854925e-01 6.30095853e-01]\n",
      " ...\n",
      " [9.48171804e-01 2.69593186e-01 3.77105889e-01 4.96529264e-03\n",
      "  9.99854925e-01 6.30095853e-01]\n",
      " [9.48171804e-01 2.69593186e-01 3.77105889e-01 4.96529264e-03\n",
      "  9.99854925e-01 6.30095853e-01]\n",
      " [1.16362016e-01 9.99125359e-01 9.99753620e-01 1.23833727e-04\n",
      "  1.00000000e+00 1.67495417e-01]] \n",
      "---------\n",
      "activated_output\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "---------\n",
      "hidden_sum\n",
      " [[ 2.90660139 -0.99668755 -0.50185052 -5.30030538  8.83811568  0.53262805]\n",
      " [-2.0273413   7.0408215   8.30838919 -8.99644696 36.05501183 -1.60348273]\n",
      " [ 2.90660139 -0.99668755 -0.50185052 -5.30030538  8.83811568  0.53262805]\n",
      " ...\n",
      " [ 2.90660139 -0.99668755 -0.50185052 -5.30030538  8.83811568  0.53262805]\n",
      " [ 2.90660139 -0.99668755 -0.50185052 -5.30030538  8.83811568  0.53262805]\n",
      " [-2.0273413   7.0408215   8.30838919 -8.99644696 36.05501183 -1.60348273]] \n",
      "---------\n",
      "weights1\n",
      " [[-2.0273413   7.0408215   8.30838919 -8.99644696 36.05501183 -1.60348273]\n",
      " [ 2.90660139 -0.99668755 -0.50185052 -5.30030538  8.83811568  0.53262805]] \n",
      "---------\n",
      "weights2\n",
      " [[30.63976698]\n",
      " [50.05641291]\n",
      " [42.99606679]\n",
      " [31.02161758]\n",
      " [30.20812652]\n",
      " [31.55071072]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find which layer is producing poor weights\n",
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output','output']\n",
    "\n",
    "[print(i + '\\n', getattr(nn,i), '\\n'+'---'*3) for i in dir(nn) if i in attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low predictions? -> Try increasing weights in either layer. Biggest positive effect from increasing weights in places where there are already high activation values. Decrease activations that correspond to negative weights and increase activations corresponding with positive weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[0.17548768]\n",
      " [0.21072884]\n",
      " [0.17548768]\n",
      " ...\n",
      " [0.17548768]\n",
      " [0.17548768]\n",
      " [0.21072884]]\n",
      "Loss: \n",
      " 0.34377643581125084\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 50---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 100---------+\n",
      "Input: \n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Actual Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Predicted Output: \n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Loss: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "# Train the nn over 100 epochs \n",
    "for i in range(100):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 50 == 0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y3)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y3 - nn.feed_forward(X)))))\n",
    "    nn.train(X,y3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy? It appears stalled at 50% (the baseline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method before moving on to part 3\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 260.243590\n",
      "         Iterations: 29\n",
      "         Function evaluations: 103\n",
      "         Gradient evaluations: 92\n"
     ]
    }
   ],
   "source": [
    "T.train(X,y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.94741061]\n",
      " [0.94777014]\n",
      " [0.94741061]\n",
      " ...\n",
      " [0.94741061]\n",
      " [0.94741061]\n",
      " [0.94777014]]\n",
      "Loss: \n",
      "0.05204871807479738\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9c078c693a85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y3 - NN.forward(X)))))\n",
    "# for regression analysis \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y3, NN.forward(X)))\n",
    "# for categorical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import keras\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>288</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "176   60    1   0       117   230    1        1      160      1      1.4   \n",
       "6     56    0   1       140   294    0        0      153      0      1.3   \n",
       "93    54    0   1       132   288    1        0      159      1      0.0   \n",
       "78    52    1   1       128   205    1        1      184      0      0.0   \n",
       "199   65    1   0       110   248    0        0      158      0      0.6   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "176      2   2     3       0  \n",
       "6        1   0     2       1  \n",
       "93       2   1     2       1  \n",
       "78       2   0     2       1  \n",
       "199      2   2     1       0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14) (303,)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,0:14]\n",
    "y = df.values[:,-1].astype('bool')\n",
    "\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = X.columns# Create the Scaler object\n",
    "scaler = StandardScaler()# Fit your data on the scaler object\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(scaled_df, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.544554\n",
       "0    0.455446\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather the baseline\n",
    "df.target.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 14) (242,) (61, 14) (61,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Person/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/Users/Person/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/sample - loss: 0.8004 - accuracy: 0.5413\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 171us/sample - loss: 0.7681 - accuracy: 0.5413\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 118us/sample - loss: 0.7484 - accuracy: 0.5413\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 114us/sample - loss: 0.7368 - accuracy: 0.5413\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.7228 - accuracy: 0.5413\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.7152 - accuracy: 0.5413\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 140us/sample - loss: 0.6991 - accuracy: 0.5413\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 136us/sample - loss: 0.7000 - accuracy: 0.5413\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 143us/sample - loss: 0.6971 - accuracy: 0.5413\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 191us/sample - loss: 0.6949 - accuracy: 0.5413\n",
      "Best: 0.5371900880632322 using {'batch_size': 30, 'epochs': 10}\n",
      "Means: 0.5206611699555531, Stdev: 0.03728131929209612 with: {'batch_size': 1, 'epochs': 10}\n",
      "Means: 0.5123966976630786, Stdev: 0.04399310816345023 with: {'batch_size': 10, 'epochs': 10}\n",
      "Means: 0.5371900880632322, Stdev: 0.0485418659995596 with: {'batch_size': 30, 'epochs': 10}\n",
      "Means: 0.48347107807466805, Stdev: 0.053135848758632954 with: {'batch_size': 60, 'epochs': 10}\n",
      "Means: 0.5000000076353057, Stdev: 0.059678298587438665 with: {'batch_size': 120, 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "#Frame the model\n",
    "\n",
    "# Plant a random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model - Sigmoid activation function for binary/boolean type\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=14, activation='sigmoid'))\n",
    "    model.add(Dropout(rate = .3))\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "#     model.add(Dropout(rate = .3))\n",
    "#     model.add(Dense(6, activation='sigmoid'))\n",
    "#     model.add(Dropout(rate = .1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# X = X_train\n",
    "# y = y_train\n",
    "\n",
    "# y_train =  keras.utils.to_categorical(y_train, 2)\n",
    "# y_test = keras.utils.to_categorical(y_test, 2)\n",
    "\n",
    "# create model; verbose? Yes\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# define the grid search parameters; Bests: batch_size = , epochs = \n",
    "param_grid = {'batch_size': [1,10,30,60,120],  \n",
    "              'epochs': [10]\n",
    "             } # These are the parameters that can be passed into the .fit() function:\n",
    "#               fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
    "#                  validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,\n",
    "#                 sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None,\n",
    "#                validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "#grid_result = grid.fit(tf.convert_to_tensor(X_train.values, dtype=tf.float64),\n",
    "#                      tf.convert_to_tensor(y_train.values, dtype=tf.float64))\n",
    "#grid_result = grid.fit(tf.convert_to_tensor(X.values, dtype = tf.float64),\n",
    "#                        tf.convert_to_tensor(y.values, dtype = tf.float64))\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results - best accuracy = \n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher batch_size yields better accuracy. Hold at 120 for next search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Person/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples\n",
      "Epoch 1/40\n",
      "242/242 [==============================] - 1s 3ms/sample - loss: 0.8574 - accuracy: 0.5413\n",
      "Epoch 2/40\n",
      "242/242 [==============================] - 0s 34us/sample - loss: 0.8405 - accuracy: 0.5413\n",
      "Epoch 3/40\n",
      "242/242 [==============================] - 0s 55us/sample - loss: 0.8354 - accuracy: 0.5413\n",
      "Epoch 4/40\n",
      "242/242 [==============================] - 0s 45us/sample - loss: 0.8315 - accuracy: 0.5413\n",
      "Epoch 5/40\n",
      "242/242 [==============================] - 0s 104us/sample - loss: 0.8232 - accuracy: 0.5413\n",
      "Epoch 6/40\n",
      "242/242 [==============================] - 0s 137us/sample - loss: 0.8075 - accuracy: 0.5413\n",
      "Epoch 7/40\n",
      "242/242 [==============================] - 0s 157us/sample - loss: 0.8047 - accuracy: 0.5413\n",
      "Epoch 8/40\n",
      "242/242 [==============================] - 0s 123us/sample - loss: 0.7961 - accuracy: 0.5413\n",
      "Epoch 9/40\n",
      "242/242 [==============================] - 0s 52us/sample - loss: 0.7920 - accuracy: 0.5413\n",
      "Epoch 10/40\n",
      "242/242 [==============================] - 0s 76us/sample - loss: 0.7774 - accuracy: 0.5413\n",
      "Epoch 11/40\n",
      "242/242 [==============================] - 0s 86us/sample - loss: 0.7716 - accuracy: 0.5413\n",
      "Epoch 12/40\n",
      "242/242 [==============================] - 0s 87us/sample - loss: 0.7631 - accuracy: 0.5413\n",
      "Epoch 13/40\n",
      "242/242 [==============================] - 0s 89us/sample - loss: 0.7536 - accuracy: 0.5413\n",
      "Epoch 14/40\n",
      "242/242 [==============================] - 0s 78us/sample - loss: 0.7534 - accuracy: 0.5413\n",
      "Epoch 15/40\n",
      "242/242 [==============================] - 0s 88us/sample - loss: 0.7486 - accuracy: 0.5413\n",
      "Epoch 16/40\n",
      "242/242 [==============================] - 0s 85us/sample - loss: 0.7459 - accuracy: 0.5413\n",
      "Epoch 17/40\n",
      "242/242 [==============================] - 0s 56us/sample - loss: 0.7419 - accuracy: 0.5413\n",
      "Epoch 18/40\n",
      "242/242 [==============================] - 0s 107us/sample - loss: 0.7389 - accuracy: 0.5413\n",
      "Epoch 19/40\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.7352 - accuracy: 0.5413\n",
      "Epoch 20/40\n",
      "242/242 [==============================] - 0s 99us/sample - loss: 0.7304 - accuracy: 0.5413\n",
      "Epoch 21/40\n",
      "242/242 [==============================] - 0s 94us/sample - loss: 0.7234 - accuracy: 0.5413\n",
      "Epoch 22/40\n",
      "242/242 [==============================] - 0s 135us/sample - loss: 0.7217 - accuracy: 0.5413\n",
      "Epoch 23/40\n",
      "242/242 [==============================] - 0s 60us/sample - loss: 0.7177 - accuracy: 0.5413\n",
      "Epoch 24/40\n",
      "242/242 [==============================] - 0s 103us/sample - loss: 0.7162 - accuracy: 0.5413\n",
      "Epoch 25/40\n",
      "242/242 [==============================] - 0s 103us/sample - loss: 0.7083 - accuracy: 0.5413\n",
      "Epoch 26/40\n",
      "242/242 [==============================] - 0s 107us/sample - loss: 0.7105 - accuracy: 0.5413\n",
      "Epoch 27/40\n",
      "242/242 [==============================] - 0s 62us/sample - loss: 0.7061 - accuracy: 0.5413\n",
      "Epoch 28/40\n",
      "242/242 [==============================] - 0s 74us/sample - loss: 0.7064 - accuracy: 0.5413\n",
      "Epoch 29/40\n",
      "242/242 [==============================] - 0s 123us/sample - loss: 0.7065 - accuracy: 0.5413\n",
      "Epoch 30/40\n",
      "242/242 [==============================] - 0s 99us/sample - loss: 0.7002 - accuracy: 0.5413\n",
      "Epoch 31/40\n",
      "242/242 [==============================] - 0s 75us/sample - loss: 0.6960 - accuracy: 0.5413\n",
      "Epoch 32/40\n",
      "242/242 [==============================] - 0s 97us/sample - loss: 0.6936 - accuracy: 0.5413\n",
      "Epoch 33/40\n",
      "242/242 [==============================] - 0s 69us/sample - loss: 0.6998 - accuracy: 0.5413\n",
      "Epoch 34/40\n",
      "242/242 [==============================] - 0s 73us/sample - loss: 0.6950 - accuracy: 0.5413\n",
      "Epoch 35/40\n",
      "242/242 [==============================] - 0s 88us/sample - loss: 0.6958 - accuracy: 0.5413\n",
      "Epoch 36/40\n",
      "242/242 [==============================] - 0s 85us/sample - loss: 0.6966 - accuracy: 0.5413\n",
      "Epoch 37/40\n",
      "242/242 [==============================] - 0s 118us/sample - loss: 0.7007 - accuracy: 0.5413\n",
      "Epoch 38/40\n",
      "242/242 [==============================] - 0s 161us/sample - loss: 0.7024 - accuracy: 0.5413\n",
      "Epoch 39/40\n",
      "242/242 [==============================] - 0s 132us/sample - loss: 0.6945 - accuracy: 0.5413\n",
      "Epoch 40/40\n",
      "242/242 [==============================] - 0s 108us/sample - loss: 0.6951 - accuracy: 0.5413\n",
      "Best: 0.5495867795688062 using {'batch_size': 120, 'epochs': 40}\n",
      "Means: 0.5123966944611762, Stdev: 0.05837654742041348 with: {'batch_size': 120, 'epochs': 10}\n",
      "Means: 0.5247933859667502, Stdev: 0.033320249562918784 with: {'batch_size': 120, 'epochs': 20}\n",
      "Means: 0.5495867795688062, Stdev: 0.05131379786131282 with: {'batch_size': 120, 'epochs': 40}\n",
      "Means: 0.5330578667565811, Stdev: 0.04052445037165304 with: {'batch_size': 120, 'epochs': 80}\n",
      "Means: 0.5289256240218139, Stdev: 0.03281792880284198 with: {'batch_size': 120, 'epochs': 160}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters; Bests: batch_size = , epochs = \n",
    "param_grid = {'batch_size': [120],  \n",
    "              'epochs': [10, 20, 40, 80, 160]\n",
    "             } # These are the parameters that can be passed into the .fit() function:\n",
    "#               fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
    "#                  validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,\n",
    "#                 sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None,\n",
    "#                validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "#grid_result = grid.fit(tf.convert_to_tensor(X_train.values, dtype=tf.float64),\n",
    "#                      tf.convert_to_tensor(y_train.values, dtype=tf.float64))\n",
    "#grid_result = grid.fit(tf.convert_to_tensor(X.values, dtype = tf.float64),\n",
    "#                        tf.convert_to_tensor(y.values, dtype = tf.float64))\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results - best accuracy = \n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the search for a good number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Person/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples\n",
      "Epoch 1/70\n",
      "242/242 [==============================] - 1s 2ms/sample - loss: 0.8953 - accuracy: 0.4587\n",
      "Epoch 2/70\n",
      "242/242 [==============================] - 0s 60us/sample - loss: 0.8738 - accuracy: 0.4587\n",
      "Epoch 3/70\n",
      "242/242 [==============================] - 0s 131us/sample - loss: 0.8697 - accuracy: 0.4587\n",
      "Epoch 4/70\n",
      "242/242 [==============================] - 0s 133us/sample - loss: 0.8584 - accuracy: 0.4587\n",
      "Epoch 5/70\n",
      "242/242 [==============================] - 0s 158us/sample - loss: 0.8427 - accuracy: 0.4587\n",
      "Epoch 6/70\n",
      "242/242 [==============================] - 0s 117us/sample - loss: 0.8273 - accuracy: 0.4587\n",
      "Epoch 7/70\n",
      "242/242 [==============================] - 0s 112us/sample - loss: 0.8129 - accuracy: 0.4587\n",
      "Epoch 8/70\n",
      "242/242 [==============================] - 0s 118us/sample - loss: 0.8169 - accuracy: 0.4587\n",
      "Epoch 9/70\n",
      "242/242 [==============================] - 0s 88us/sample - loss: 0.8040 - accuracy: 0.4587\n",
      "Epoch 10/70\n",
      "242/242 [==============================] - 0s 101us/sample - loss: 0.7894 - accuracy: 0.4587\n",
      "Epoch 11/70\n",
      "242/242 [==============================] - 0s 68us/sample - loss: 0.7821 - accuracy: 0.4587\n",
      "Epoch 12/70\n",
      "242/242 [==============================] - 0s 86us/sample - loss: 0.7804 - accuracy: 0.4587\n",
      "Epoch 13/70\n",
      "242/242 [==============================] - 0s 75us/sample - loss: 0.7752 - accuracy: 0.4587\n",
      "Epoch 14/70\n",
      "242/242 [==============================] - 0s 100us/sample - loss: 0.7643 - accuracy: 0.4587\n",
      "Epoch 15/70\n",
      "242/242 [==============================] - 0s 94us/sample - loss: 0.7532 - accuracy: 0.4587\n",
      "Epoch 16/70\n",
      "242/242 [==============================] - 0s 130us/sample - loss: 0.7505 - accuracy: 0.4587\n",
      "Epoch 17/70\n",
      "242/242 [==============================] - 0s 62us/sample - loss: 0.7375 - accuracy: 0.4587\n",
      "Epoch 18/70\n",
      "242/242 [==============================] - 0s 75us/sample - loss: 0.7408 - accuracy: 0.4587\n",
      "Epoch 19/70\n",
      "242/242 [==============================] - 0s 166us/sample - loss: 0.7218 - accuracy: 0.4587\n",
      "Epoch 20/70\n",
      "242/242 [==============================] - 0s 88us/sample - loss: 0.7211 - accuracy: 0.4587\n",
      "Epoch 21/70\n",
      "242/242 [==============================] - 0s 128us/sample - loss: 0.7274 - accuracy: 0.4628\n",
      "Epoch 22/70\n",
      "242/242 [==============================] - 0s 85us/sample - loss: 0.7364 - accuracy: 0.4587\n",
      "Epoch 23/70\n",
      "242/242 [==============================] - 0s 73us/sample - loss: 0.7221 - accuracy: 0.4711\n",
      "Epoch 24/70\n",
      "242/242 [==============================] - 0s 81us/sample - loss: 0.7104 - accuracy: 0.4587\n",
      "Epoch 25/70\n",
      "242/242 [==============================] - 0s 74us/sample - loss: 0.7217 - accuracy: 0.4545\n",
      "Epoch 26/70\n",
      "242/242 [==============================] - 0s 69us/sample - loss: 0.7139 - accuracy: 0.4752\n",
      "Epoch 27/70\n",
      "242/242 [==============================] - 0s 81us/sample - loss: 0.7039 - accuracy: 0.4793\n",
      "Epoch 28/70\n",
      "242/242 [==============================] - 0s 127us/sample - loss: 0.7113 - accuracy: 0.4463\n",
      "Epoch 29/70\n",
      "242/242 [==============================] - 0s 109us/sample - loss: 0.7146 - accuracy: 0.4545\n",
      "Epoch 30/70\n",
      "242/242 [==============================] - 0s 75us/sample - loss: 0.7043 - accuracy: 0.4463\n",
      "Epoch 31/70\n",
      "242/242 [==============================] - 0s 107us/sample - loss: 0.7015 - accuracy: 0.4876\n",
      "Epoch 32/70\n",
      "242/242 [==============================] - 0s 100us/sample - loss: 0.7026 - accuracy: 0.4421\n",
      "Epoch 33/70\n",
      "242/242 [==============================] - 0s 68us/sample - loss: 0.6958 - accuracy: 0.4835\n",
      "Epoch 34/70\n",
      "242/242 [==============================] - 0s 70us/sample - loss: 0.6902 - accuracy: 0.5207\n",
      "Epoch 35/70\n",
      "242/242 [==============================] - 0s 78us/sample - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 36/70\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.6994 - accuracy: 0.4752\n",
      "Epoch 37/70\n",
      "242/242 [==============================] - 0s 33us/sample - loss: 0.6973 - accuracy: 0.4835\n",
      "Epoch 38/70\n",
      "242/242 [==============================] - 0s 29us/sample - loss: 0.7008 - accuracy: 0.4504\n",
      "Epoch 39/70\n",
      "242/242 [==============================] - 0s 34us/sample - loss: 0.6897 - accuracy: 0.5165\n",
      "Epoch 40/70\n",
      "242/242 [==============================] - 0s 30us/sample - loss: 0.6861 - accuracy: 0.5331\n",
      "Epoch 41/70\n",
      "242/242 [==============================] - 0s 28us/sample - loss: 0.6933 - accuracy: 0.5289\n",
      "Epoch 42/70\n",
      "242/242 [==============================] - 0s 30us/sample - loss: 0.6924 - accuracy: 0.5537\n",
      "Epoch 43/70\n",
      "242/242 [==============================] - 0s 195us/sample - loss: 0.6861 - accuracy: 0.5537\n",
      "Epoch 44/70\n",
      "242/242 [==============================] - 0s 144us/sample - loss: 0.6865 - accuracy: 0.5702\n",
      "Epoch 45/70\n",
      "242/242 [==============================] - 0s 119us/sample - loss: 0.6902 - accuracy: 0.5289\n",
      "Epoch 46/70\n",
      "242/242 [==============================] - 0s 72us/sample - loss: 0.6846 - accuracy: 0.5372\n",
      "Epoch 47/70\n",
      "242/242 [==============================] - 0s 83us/sample - loss: 0.6895 - accuracy: 0.5372\n",
      "Epoch 48/70\n",
      "242/242 [==============================] - 0s 89us/sample - loss: 0.6917 - accuracy: 0.5455\n",
      "Epoch 49/70\n",
      "242/242 [==============================] - 0s 97us/sample - loss: 0.6898 - accuracy: 0.5826\n",
      "Epoch 50/70\n",
      "242/242 [==============================] - 0s 80us/sample - loss: 0.6851 - accuracy: 0.5868\n",
      "Epoch 51/70\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.6908 - accuracy: 0.5000\n",
      "Epoch 52/70\n",
      "242/242 [==============================] - 0s 55us/sample - loss: 0.6933 - accuracy: 0.5331\n",
      "Epoch 53/70\n",
      "242/242 [==============================] - 0s 68us/sample - loss: 0.6943 - accuracy: 0.5041\n",
      "Epoch 54/70\n",
      "242/242 [==============================] - 0s 97us/sample - loss: 0.6907 - accuracy: 0.5537\n",
      "Epoch 55/70\n",
      "242/242 [==============================] - 0s 143us/sample - loss: 0.6894 - accuracy: 0.5041\n",
      "Epoch 56/70\n",
      "242/242 [==============================] - 0s 153us/sample - loss: 0.6932 - accuracy: 0.5372\n",
      "Epoch 57/70\n",
      "242/242 [==============================] - 0s 120us/sample - loss: 0.6910 - accuracy: 0.5496\n",
      "Epoch 58/70\n",
      "242/242 [==============================] - 0s 129us/sample - loss: 0.6921 - accuracy: 0.5455\n",
      "Epoch 59/70\n",
      "242/242 [==============================] - 0s 83us/sample - loss: 0.6808 - accuracy: 0.5620\n",
      "Epoch 60/70\n",
      "242/242 [==============================] - 0s 79us/sample - loss: 0.6916 - accuracy: 0.5331\n",
      "Epoch 61/70\n",
      "242/242 [==============================] - 0s 81us/sample - loss: 0.6850 - accuracy: 0.5289\n",
      "Epoch 62/70\n",
      "242/242 [==============================] - 0s 82us/sample - loss: 0.6851 - accuracy: 0.5579\n",
      "Epoch 63/70\n",
      "242/242 [==============================] - 0s 112us/sample - loss: 0.6917 - accuracy: 0.5000\n",
      "Epoch 64/70\n",
      "242/242 [==============================] - 0s 78us/sample - loss: 0.6859 - accuracy: 0.5413\n",
      "Epoch 65/70\n",
      "242/242 [==============================] - 0s 50us/sample - loss: 0.6922 - accuracy: 0.5537\n",
      "Epoch 66/70\n",
      "242/242 [==============================] - 0s 73us/sample - loss: 0.6811 - accuracy: 0.5744\n",
      "Epoch 67/70\n",
      "242/242 [==============================] - 0s 70us/sample - loss: 0.6879 - accuracy: 0.5537\n",
      "Epoch 68/70\n",
      "242/242 [==============================] - 0s 85us/sample - loss: 0.6865 - accuracy: 0.5207\n",
      "Epoch 69/70\n",
      "242/242 [==============================] - 0s 72us/sample - loss: 0.6900 - accuracy: 0.5455\n",
      "Epoch 70/70\n",
      "242/242 [==============================] - 0s 105us/sample - loss: 0.6811 - accuracy: 0.5868\n",
      "Best: 0.5413223231626936 using {'batch_size': 120, 'epochs': 70}\n",
      "Means: 0.5330578485303674, Stdev: 0.03580414154147596 with: {'batch_size': 120, 'epochs': 30}\n",
      "Means: 0.5165289181076791, Stdev: 0.03810179130397241 with: {'batch_size': 120, 'epochs': 40}\n",
      "Means: 0.5165289128122251, Stdev: 0.025944667511621847 with: {'batch_size': 120, 'epochs': 50}\n",
      "Means: 0.5371900956985379, Stdev: 0.04545506452499293 with: {'batch_size': 120, 'epochs': 60}\n",
      "Means: 0.5413223231626936, Stdev: 0.045861768046698635 with: {'batch_size': 120, 'epochs': 70}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters; Bests: batch_size = , epochs = \n",
    "param_grid = {'batch_size': [120],  \n",
    "              'epochs': [30, 40, 50, 60, 70]\n",
    "             } # These are the parameters that can be passed into the .fit() function:\n",
    "#               fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
    "#                  validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,\n",
    "#                 sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None,\n",
    "#                validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
    "\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "#grid_result = grid.fit(tf.convert_to_tensor(X_train.values, dtype=tf.float64),\n",
    "#                      tf.convert_to_tensor(y_train.values, dtype=tf.float64))\n",
    "#grid_result = grid.fit(tf.convert_to_tensor(X.values, dtype = tf.float64),\n",
    "#                        tf.convert_to_tensor(y.values, dtype = tf.float64))\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results - best accuracy = \n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better results with number of epochs set to 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (242, 2) was passed for an output of shape (None, 1) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-04976dd10f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_binary_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_binary_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    517\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    518\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (242, 2) was passed for an output of shape (None, 1) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.layers\n",
    "\n",
    "y_binary_train = keras.utils.to_categorical(y_train)\n",
    "y_binary_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='sigmoid', input_shape=(14,)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_binary_train, batch_size=120, epochs=70, verbose=False, validation_split=.1)\n",
    "loss, accuracy  = model.evaluate(X_test, y_binary_test, verbose=False)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show();\n",
    "\n",
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
